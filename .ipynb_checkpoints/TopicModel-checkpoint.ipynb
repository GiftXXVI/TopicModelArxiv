{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.016*\"aphunzitsi\" + 0.013*\"bingu\" + 0.011*\"boma\" + 0.011*\"statu\" + '\n",
      "  '0.009*\"presid\" + 0.007*\"unveil\" + 0.006*\"mutharika\" + 0.006*\"kampeni\" + '\n",
      "  '0.006*\"the\" + 0.006*\"tum\"'),\n",
      " (1,\n",
      "  '0.007*\"anthu\" + 0.006*\"said\" + 0.005*\"dream\" + 0.005*\"inn\" + 0.005*\"say\" + '\n",
      "  '0.004*\"adatero\" + 0.004*\"gogo\" + 0.004*\"adati\" + 0.004*\"imposs\" + '\n",
      "  '0.004*\"makolo\"'),\n",
      " (2,\n",
      "  '0.013*\"madzi\" + 0.012*\"mbewu\" + 0.010*\"ulimiwu\" + 0.010*\"kodi\" + '\n",
      "  '0.008*\"mvula\" + 0.008*\"nanga\" + 0.007*\"ndipo\" + 0.005*\"chimanga\" + '\n",
      "  '0.005*\"moter\" + 0.005*\"komanso\"'),\n",
      " (3,\n",
      "  '0.012*\"qualifi\" + 0.012*\"flame\" + 0.007*\"executive_committe\" + 0.007*\"cup\" '\n",
      "  '+ 0.007*\"respons\" + 0.007*\"said\" + 0.005*\"morocco\" + 0.005*\"submit\" + '\n",
      "  '0.005*\"made\" + 0.005*\"waiting_govern\"'),\n",
      " (4,\n",
      "  '0.011*\"candid\" + 0.010*\"said\" + 0.008*\"mbewu\" + 0.007*\"team\" + '\n",
      "  '0.006*\"malawi\" + 0.006*\"govern\" + 0.005*\"tackl\" + 0.005*\"would\" + '\n",
      "  '0.005*\"debat\" + 0.004*\"issu\"'),\n",
      " (5,\n",
      "  '0.017*\"grief\" + 0.013*\"loss\" + 0.010*\"person\" + 0.008*\"life\" + 0.008*\"lose\" '\n",
      "  '+ 0.008*\"child\" + 0.008*\"what\" + 0.007*\"deep\" + 0.007*\"love\" + '\n",
      "  '0.005*\"come\"'),\n",
      " (6,\n",
      "  '0.025*\"admarc\" + 0.018*\"mera\" + 0.012*\"billion\" + 0.012*\"maiz\" + '\n",
      "  '0.010*\"million\" + 0.008*\"loan\" + 0.008*\"psf\" + 0.006*\"amount\" + '\n",
      "  '0.006*\"also\" + 0.006*\"but\"'),\n",
      " (7,\n",
      "  '0.000*\"asavutik\" + 0.000*\"ayi\" + 0.000*\"alangizi\" + 0.000*\"alipo\" + '\n",
      "  '0.000*\"amalumikiza\" + 0.000*\"amayambiratu\" + 0.000*\"andithand\" + '\n",
      "  '0.000*\"anji_omw\" + 0.000*\"aja\" + 0.000*\"banki\"'),\n",
      " (8,\n",
      "  '0.015*\"rvg\" + 0.010*\"meet\" + 0.010*\"executive_committe\" + 0.010*\"said\" + '\n",
      "  '0.008*\"footbal\" + 0.008*\"two\" + 0.008*\"recommend\" + 0.008*\"howev\" + '\n",
      "  '0.008*\"also\" + 0.008*\"technic\"'),\n",
      " (9,\n",
      "  '0.010*\"manyowa\" + 0.008*\"nkhaniyo\" + 0.008*\"ruckaya\" + 0.008*\"mwanayo\" + '\n",
      "  '0.008*\"abdul\" + 0.007*\"komanso\" + 0.007*\"mwamuna_wak\" + 0.007*\"ndipo\" + '\n",
      "  '0.007*\"adatero\" + 0.007*\"mlandu\"')]\n",
      "\n",
      "Model Perplexity:  -7.765045587525254\n",
      "\n",
      "Model Coherence:  0.5912215500279606\n"
     ]
    }
   ],
   "source": [
    "import xmltodict\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import string\n",
    "import nltk\n",
    "import gensim\n",
    "response = requests.get('https://www.mwnation.com/feed/?paged=501')\n",
    "doc = xmltodict.parse(response.text)\n",
    "d = doc['rss']['channel']['item']\n",
    "docs = list()\n",
    "for i in d:\n",
    "    #print(i)\n",
    "    content = i['content:encoded']\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "    docs.append(soup.get_text())\n",
    "#print(docs)\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "#return tokens\n",
    "token_docs = [word_tokenize(i.encode('ascii',errors='ignore').decode()) for i in docs]\n",
    "\n",
    "word_docs = list()\n",
    "#remove punctuation\n",
    "for token_doc in token_docs:\n",
    "    word_docs.append([word for word in token_doc if word.isalpha()])\n",
    "from nltk.corpus import stopwords\n",
    "extra_stops = ['ine','wanga','inemwini','ife','zathu','tokha','inu','anu','wanu','nokha','iye','ake','iyemwini','izo',\n",
    "'iwo','awo','zawo','iwowo','chani','zomwe','who','amene','izi','kuti','amenewo','ndili','ndi','anali','khalani','akhala',\n",
    "'kukhala','ali','chitani','amatero','anatero','akuchita','a','koma','ngati','kapena','chifukwa','monga','mpaka','pomwe',\n",
    "'wa','at','za','motsutsa','pakati','mu','kudzera','nthawi','kale','pambuyo','pamwambapa','pansipa','kuchokera',\"m'mwamba\",\n",
    "'pansi','kunja','pa','zochotsa','kwatha','kachiwiri','kupitirira','ndiye','kamodzi','Pano','Apo','liti','pati','bwanji',\n",
    "'zonse','chilichonse','onse','aliyense','ochepa','Zambiri','ambiri','ena','zotere','ayi','ngakhale','kokha','changa',\n",
    "'momwemonso','choncho','kuposa','nawonso','kwambiri','angathe','chifuniro','basi','don','ayenera','tsopano',]\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.update(extra_stops)\n",
    "nostw_docs = list()\n",
    "#remove stopwords\n",
    "for word_doc in word_docs:\n",
    "    nostw_docs.append([w for w in word_doc if not w in stop_words])\n",
    "no1ltw_docs = list()\n",
    "#remove 1 letter words and lowercase\n",
    "for nostw_doc in nostw_docs:\n",
    "    no1ltw_docs.append([w.lower() for w in nostw_doc if not len(w) <= 2])\n",
    "\n",
    "bigram = gensim.models.Phrases(no1ltw_docs, min_count=1, threshold=1)\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "\n",
    "bigram_docs = list()\n",
    "for no1ltw_doc in no1ltw_docs:\n",
    "    bigram_docs.append(bigram_mod[no1ltw_doc])\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemma_docs = list()\n",
    "for bigram_doc in bigram_docs:\n",
    "    lemma_docs.append([lemmatizer.lemmatize(word) for word in bigram_doc])\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_docs = list()\n",
    "for lemma_doc in lemma_docs:\n",
    "    stemmed_docs.append([stemmer.stem(word) for word in lemma_doc])\n",
    "    \n",
    "\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "id2word = corpora.Dictionary(stemmed_docs)\n",
    "corpus = [id2word.doc2bow(text) for text in stemmed_docs]\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,id2word=id2word,num_topics=10,chunksize = 50,update_every=1,\n",
    "                                           passes=10000,alpha='auto',per_word_topics=True)\n",
    "from pprint import pprint\n",
    "pprint(lda_model.print_topics())\n",
    "# Compute Perplexity\n",
    "print('\\nModel Perplexity: ', lda_model.log_perplexity(corpus))  #lower is better\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=stemmed_docs, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nModel Coherence: ', coherence_lda)#higher is better\n",
    "\n",
    "#import pyLDAvis\n",
    "#import pyLDAvis.gensim\n",
    "#import matplotlib.pyplot as plotter\n",
    "#%matplotlib inline\n",
    "#pyLDAvis.enable_notebook()\n",
    "#vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "#vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
