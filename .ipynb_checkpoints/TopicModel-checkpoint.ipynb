{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.031*\"and\" + 0.022*\"partial_corr\" + 0.018*\"identify\" + 0.014*\"missing\" + '\n",
      "  '0.014*\"patterns\" + 0.014*\"the\" + 0.009*\"for\" + 0.009*\"graphical\" + '\n",
      "  '0.009*\"insights_into\" + 0.009*\"temperature_partial\"'),\n",
      " (1,\n",
      "  '0.047*\"the\" + 0.020*\"with\" + 0.016*\"and\" + 0.016*\"for\" + 0.012*\"that_the\" + '\n",
      "  '0.012*\"that\" + 0.012*\"space\" + 0.012*\"such\" + 0.008*\"are\" + '\n",
      "  '0.008*\"composition\"'),\n",
      " (2,\n",
      "  '0.057*\"the\" + 0.038*\"and\" + 0.019*\"corr\" + 0.012*\"that\" + 0.012*\"data\" + '\n",
      "  '0.012*\"architecture\" + 0.012*\"this\" + 0.012*\"repository\" + 0.008*\"for\" + '\n",
      "  '0.008*\"with_the\"'),\n",
      " (3,\n",
      "  '0.040*\"the\" + 0.026*\"and\" + 0.015*\"for_the\" + 0.015*\"accuracy\" + '\n",
      "  '0.011*\"dblp\" + 0.011*\"arabic\" + 0.011*\"research\" + 0.011*\"languages\" + '\n",
      "  '0.011*\"with\" + 0.011*\"recognition\"'),\n",
      " (4,\n",
      "  '0.017*\"representation\" + 0.017*\"the\" + 0.017*\"that\" + 0.011*\"for\" + '\n",
      "  '0.011*\"show\" + 0.011*\"algorithm\" + 0.011*\"the_memory\" + 0.011*\"and\" + '\n",
      "  '0.011*\"representations\" + 0.006*\"relatively\"'),\n",
      " (5,\n",
      "  '0.032*\"the\" + 0.019*\"and\" + 0.013*\"alamos\" + 0.013*\"some\" + '\n",
      "  '0.013*\"archives\" + 0.013*\"its\" + 0.013*\"the_los\" + 0.013*\"corr\" + '\n",
      "  '0.007*\"implicit\" + 0.007*\"intended\"'),\n",
      " (6,\n",
      "  '0.045*\"and\" + 0.018*\"the\" + 0.015*\"with\" + 0.012*\"corr_delta\" + 0.009*\"for\" '\n",
      "  '+ 0.009*\"circadian_rhythm\" + 0.009*\"was\" + 0.009*\"delta\" + '\n",
      "  '0.009*\"recordings\" + 0.009*\"our\"'),\n",
      " (7,\n",
      "  '0.023*\"quantum\" + 0.012*\"using\" + 0.012*\"simulated\" + 0.012*\"proposed\" + '\n",
      "  '0.012*\"for\" + 0.012*\"pattern\" + 0.012*\"holography\" + 0.012*\"associative\" + '\n",
      "  '0.012*\"which\" + 0.012*\"recognition\"'),\n",
      " (8,\n",
      "  '0.023*\"degree\" + 0.012*\"which_are\" + 0.012*\"weighted\" + 0.012*\"invariant\" + '\n",
      "  '0.012*\"holomorphic\" + 0.012*\"projective\" + 0.012*\"space\" + 0.012*\"one\" + '\n",
      "  '0.012*\"given\" + 0.012*\"bounding\"'),\n",
      " (9,\n",
      "  '0.054*\"the\" + 0.025*\"and\" + 0.019*\"matrices\" + 0.013*\"and_use\" + '\n",
      "  '0.013*\"primary_cyclic\" + 0.013*\"not\" + 0.013*\"index\" + 0.007*\"algebras\" + '\n",
      "  '0.007*\"theorem\" + 0.007*\"theory\"')]\n",
      "\n",
      "Model Perplexity:  -6.695803559489201\n",
      "\n",
      "Model Coherence:  0.4876258338417568\n"
     ]
    }
   ],
   "source": [
    "import xmltodict\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import string\n",
    "import nltk\n",
    "import gensim\n",
    "response = requests.get('http://export.arxiv.org/api/query?search_query=all:corr&amp;id_list=&amp;start=0&amp;max_results=25')\n",
    "doc = xmltodict.parse(response.text)\n",
    "#d = doc['rss']['channel']['item']\n",
    "d = doc['feed']['entry']\n",
    "docs = list()\n",
    "for i in d:\n",
    "    docs.append(i['summary'])\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "#return tokens\n",
    "token_docs = [word_tokenize(i.encode('ascii',errors='ignore').decode()) for i in docs]\n",
    "\n",
    "word_docs = list()\n",
    "#remove punctuation\n",
    "for token_doc in token_docs:\n",
    "    word_docs.append([word for word in token_doc if word.isalpha()])\n",
    "no1ltw_docs = list()\n",
    "#remove 1 letter words and lowercase\n",
    "for word_doc in word_docs:\n",
    "    no1ltw_docs.append([w.lower() for w in word_doc if not len(w) <= 2])\n",
    "\n",
    "bigram = gensim.models.Phrases(no1ltw_docs, min_count=1, threshold=1)\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "\n",
    "bigram_docs = list()\n",
    "for no1ltw_doc in no1ltw_docs:\n",
    "    bigram_docs.append(bigram_mod[no1ltw_doc])\n",
    "    \n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "id2word = corpora.Dictionary(bigram_docs)\n",
    "corpus = [id2word.doc2bow(text) for text in bigram_docs]\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,id2word=id2word,num_topics=10,chunksize = 50,update_every=1,\n",
    "                                           passes=10000,alpha='auto',per_word_topics=True)\n",
    "from pprint import pprint\n",
    "pprint(lda_model.print_topics())\n",
    "# Compute Perplexity\n",
    "print('\\nModel Perplexity: ', lda_model.log_perplexity(corpus))  #lower is better\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=bigram_docs, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nModel Coherence: ', coherence_lda)#higher is better\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
